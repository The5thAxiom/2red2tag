{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess audio data\n",
    "def load_and_preprocess_data(data_dir, classes, target_shape=(128, 128)):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith(\".wav\"):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "                # Perform preprocessing (e.g., convert to Mel spectrogram and resize)\n",
    "                mel_spectrogram = librosa.feature.melspectrogram(\n",
    "                    y=audio_data, sr=sample_rate\n",
    "                )\n",
    "                mel_spectrogram = resize(\n",
    "                    np.expand_dims(mel_spectrogram, axis=-1), target_shape\n",
    "                )\n",
    "                data.append(mel_spectrogram)\n",
    "                labels.append(i)\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shria\\Anaconda3\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1891\n",
      "  warnings.warn(\n",
      "c:\\Users\\shria\\Anaconda3\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n",
      "c:\\Users\\shria\\Anaconda3\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1837\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"D:/Programming/VoiceRecog/for-norm/training\"\n",
    "classes = [\"fake\", \"real\"]\n",
    "# Split data into training and testing sets\n",
    "data, labels = load_and_preprocess_data(data_dir, classes)\n",
    "labels = to_categorical(\n",
    "    labels, num_classes=len(classes)\n",
    ")  # Convert labels to one-hot encoding\n",
    "X_train, y_train = data, labels\n",
    "\n",
    "\n",
    "data_dir2 = \"D:/Programming/VoiceRecog/for-norm/testing\"\n",
    "classes2 = [\"fake\", \"real\"]\n",
    "data2, labels2 = load_and_preprocess_data(data_dir2, classes2)\n",
    "labels2 = to_categorical(labels2, num_classes=len(classes2))\n",
    "X_test, y_test = data2, labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into training and testing sets\n",
    "# data, labels = load_and_preprocess_data(data_dir, classes)\n",
    "# labels = to_categorical(labels, num_classes=len(classes))  # Convert labels to one-hot encoding\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train[0].shape\n",
    "input_layer = Input(shape=input_shape)\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\")(input_layer)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "output_layer = Dense(len(classes), activation=\"softmax\")(x)\n",
    "model = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1684/1684 [==============================] - 472s 280ms/step - loss: 0.4197 - accuracy: 0.9104 - val_loss: 0.9372 - val_accuracy: 0.7451\n",
      "Epoch 2/20\n",
      "1684/1684 [==============================] - 455s 270ms/step - loss: 0.1431 - accuracy: 0.9474 - val_loss: 0.6565 - val_accuracy: 0.8215\n",
      "Epoch 3/20\n",
      "1684/1684 [==============================] - 449s 267ms/step - loss: 0.0967 - accuracy: 0.9649 - val_loss: 0.7881 - val_accuracy: 0.8295\n",
      "Epoch 4/20\n",
      "1684/1684 [==============================] - 453s 269ms/step - loss: 0.0719 - accuracy: 0.9762 - val_loss: 0.3236 - val_accuracy: 0.8988\n",
      "Epoch 5/20\n",
      "1684/1684 [==============================] - 459s 272ms/step - loss: 0.0509 - accuracy: 0.9822 - val_loss: 0.2980 - val_accuracy: 0.9163\n",
      "Epoch 6/20\n",
      "1684/1684 [==============================] - 459s 273ms/step - loss: 0.0356 - accuracy: 0.9875 - val_loss: 0.3231 - val_accuracy: 0.9184\n",
      "Epoch 7/20\n",
      "1684/1684 [==============================] - 467s 278ms/step - loss: 0.0296 - accuracy: 0.9901 - val_loss: 0.3285 - val_accuracy: 0.9057\n",
      "Epoch 8/20\n",
      "1684/1684 [==============================] - 464s 275ms/step - loss: 0.0238 - accuracy: 0.9929 - val_loss: 0.6615 - val_accuracy: 0.8943\n",
      "Epoch 9/20\n",
      "1684/1684 [==============================] - 459s 272ms/step - loss: 0.0154 - accuracy: 0.9952 - val_loss: 0.5358 - val_accuracy: 0.9126\n",
      "Epoch 10/20\n",
      "1684/1684 [==============================] - 449s 266ms/step - loss: 0.0192 - accuracy: 0.9941 - val_loss: 0.5320 - val_accuracy: 0.9111\n",
      "Epoch 11/20\n",
      "1684/1684 [==============================] - 448s 266ms/step - loss: 0.0147 - accuracy: 0.9957 - val_loss: 1.1574 - val_accuracy: 0.8798\n",
      "Epoch 12/20\n",
      "1684/1684 [==============================] - 457s 271ms/step - loss: 0.0147 - accuracy: 0.9960 - val_loss: 0.9074 - val_accuracy: 0.8966\n",
      "Epoch 13/20\n",
      "1684/1684 [==============================] - 457s 271ms/step - loss: 0.0159 - accuracy: 0.9956 - val_loss: 0.9802 - val_accuracy: 0.8953\n",
      "Epoch 14/20\n",
      "1684/1684 [==============================] - 461s 274ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 0.9863 - val_accuracy: 0.9053\n",
      "Epoch 15/20\n",
      "1684/1684 [==============================] - 470s 279ms/step - loss: 0.0112 - accuracy: 0.9968 - val_loss: 0.4891 - val_accuracy: 0.9307\n",
      "Epoch 16/20\n",
      "1684/1684 [==============================] - 466s 277ms/step - loss: 0.0121 - accuracy: 0.9973 - val_loss: 0.4705 - val_accuracy: 0.9258\n",
      "Epoch 17/20\n",
      "1684/1684 [==============================] - 466s 277ms/step - loss: 0.0106 - accuracy: 0.9976 - val_loss: 0.4814 - val_accuracy: 0.9256\n",
      "Epoch 18/20\n",
      "1684/1684 [==============================] - 467s 277ms/step - loss: 0.0094 - accuracy: 0.9978 - val_loss: 1.7275 - val_accuracy: 0.8776\n",
      "Epoch 19/20\n",
      "1684/1684 [==============================] - 471s 280ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.5051 - val_accuracy: 0.9247\n",
      "Epoch 20/20\n",
      "1684/1684 [==============================] - 453s 269ms/step - loss: 0.0097 - accuracy: 0.9980 - val_loss: 0.8888 - val_accuracy: 0.9316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e8bd4b21c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931592583656311\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"audio_classification_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 583ms/step\n",
      "Class: fake, Probability: 0.0000\n",
      "Class: real, Probability: 1.0000\n",
      "The audio is classified as: real\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = load_model(\"audio_classification_model.h5\")\n",
    "\n",
    "# Define the target shape for input spectrograms\n",
    "target_shape = (128, 128)\n",
    "\n",
    "# Define your class labels\n",
    "classes = [\"fake\", \"real\"]\n",
    "\n",
    "\n",
    "# Function to preprocess and classify an audio file\n",
    "def test_audio(file_path, model):\n",
    "    # Load and preprocess the audio file\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "    mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n",
    "    mel_spectrogram = tf.reshape(mel_spectrogram, (1,) + target_shape + (1,))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(mel_spectrogram)\n",
    "\n",
    "    # Get the class probabilities\n",
    "    class_probabilities = predictions[0]\n",
    "\n",
    "    # Get the predicted class index\n",
    "    predicted_class_index = np.argmax(class_probabilities)\n",
    "\n",
    "    return class_probabilities, predicted_class_index\n",
    "# Test an audio file\n",
    "test_audio_file = \"D:/Programming/VoiceRecog/for-norm/training/real/file6.wav_16k.wav_norm.wav_mono.wav_silence.wav\"\n",
    "class_probabilities, predicted_class_index = test_audio(test_audio_file, model)\n",
    "\n",
    "# Display results for all classes\n",
    "for i, class_label in enumerate(classes):\n",
    "    probability = class_probabilities[i]\n",
    "    print(f\"Class: {class_label}, Probability: {probability:.4f}\")\n",
    "\n",
    "# Calculate and display the predicted class and accuracy\n",
    "predicted_class = classes[predicted_class_index]\n",
    "accuracy = class_probabilities[predicted_class_index]\n",
    "print(f\"The audio is classified as: {predicted_class}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
