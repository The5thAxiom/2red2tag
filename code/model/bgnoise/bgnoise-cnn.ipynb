{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa as lr\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, models\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_extract_features(audio_path, max_length=345):\n",
    "    audio_data, _ = lr.load(audio_path, sr=None)\n",
    "    mfccs = lr.feature.mfcc(y=audio_data, sr=44100, n_mfcc=40)\n",
    "    if mfccs.shape[1] < max_length:\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, max_length - mfccs.shape[1])), mode='constant')\n",
    "    elif mfccs.shape[1] > max_length:\n",
    "        mfccs = mfccs[:, :max_length]\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 345)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_extract_features('../../data/snsd/generated/noise_high/clnsp0-high.wav').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(3, activation='softmax'))  # Output layer with 3 classes: low, medium, high\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2], 1)  # Input shape for CNN\n",
    "    X_train = X_train.reshape(X_train.shape[0], *input_shape)\n",
    "    X_val = X_val.reshape(X_val.shape[0], *input_shape)\n",
    "    model = build_cnn_model(input_shape)\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    features = []\n",
    "    files = pickle.load(open('files-dict-list.pkl', 'rb'))\n",
    "    for f in files:\n",
    "        features.append(cnn_extract_features(f['path']))\n",
    "    # Convert features to numpy array\n",
    "    X = np.array(features)\n",
    "    y = np.array([\n",
    "        0 if x['label'] == 'low'\n",
    "        else 1 if x['label'] == 'medium'\n",
    "        else 2\n",
    "        for x in files\n",
    "    ])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 409ms/step - accuracy: 0.3658 - loss: 5.7504 - val_accuracy: 0.4621 - val_loss: 1.0616\n",
      "Epoch 2/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 347ms/step - accuracy: 0.4578 - loss: 1.0486 - val_accuracy: 0.4015 - val_loss: 1.0761\n",
      "Epoch 3/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 379ms/step - accuracy: 0.4492 - loss: 1.0263 - val_accuracy: 0.4667 - val_loss: 0.9951\n",
      "Epoch 4/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 381ms/step - accuracy: 0.4910 - loss: 0.9821 - val_accuracy: 0.4091 - val_loss: 1.0029\n",
      "Epoch 5/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 405ms/step - accuracy: 0.4962 - loss: 0.9550 - val_accuracy: 0.4455 - val_loss: 0.9879\n",
      "Epoch 6/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 401ms/step - accuracy: 0.5245 - loss: 0.9360 - val_accuracy: 0.4667 - val_loss: 0.9822\n",
      "Epoch 7/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 386ms/step - accuracy: 0.5145 - loss: 0.9188 - val_accuracy: 0.4076 - val_loss: 1.0005\n",
      "Epoch 8/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 381ms/step - accuracy: 0.5495 - loss: 0.9009 - val_accuracy: 0.4727 - val_loss: 0.9894\n",
      "Epoch 9/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 389ms/step - accuracy: 0.5700 - loss: 0.8848 - val_accuracy: 0.4394 - val_loss: 0.9887\n",
      "Epoch 10/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 374ms/step - accuracy: 0.5478 - loss: 0.8596 - val_accuracy: 0.4288 - val_loss: 0.9871\n",
      "Epoch 11/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 375ms/step - accuracy: 0.5583 - loss: 0.8667 - val_accuracy: 0.4424 - val_loss: 0.9969\n",
      "Epoch 12/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 373ms/step - accuracy: 0.6066 - loss: 0.8120 - val_accuracy: 0.4167 - val_loss: 1.0317\n",
      "Epoch 13/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 379ms/step - accuracy: 0.5981 - loss: 0.8081 - val_accuracy: 0.4758 - val_loss: 1.0293\n",
      "Epoch 14/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 380ms/step - accuracy: 0.6486 - loss: 0.7406 - val_accuracy: 0.4136 - val_loss: 1.0496\n",
      "Epoch 15/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 381ms/step - accuracy: 0.6107 - loss: 0.7651 - val_accuracy: 0.4515 - val_loss: 1.0740\n",
      "Epoch 16/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 377ms/step - accuracy: 0.6637 - loss: 0.7247 - val_accuracy: 0.4288 - val_loss: 1.1406\n",
      "Epoch 17/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 383ms/step - accuracy: 0.6744 - loss: 0.6938 - val_accuracy: 0.3985 - val_loss: 1.2053\n",
      "Epoch 18/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 385ms/step - accuracy: 0.6883 - loss: 0.6911 - val_accuracy: 0.4061 - val_loss: 1.1358\n",
      "Epoch 19/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 394ms/step - accuracy: 0.6730 - loss: 0.7170 - val_accuracy: 0.4364 - val_loss: 1.2217\n",
      "Epoch 20/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 379ms/step - accuracy: 0.7135 - loss: 0.6480 - val_accuracy: 0.3879 - val_loss: 1.2279\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = train_model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('bgnoise_cnn_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(extract)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
